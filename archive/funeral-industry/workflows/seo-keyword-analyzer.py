#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å’Œå…‰è‘¬å„€ç¤¾ SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆå‹•çš„SERPåˆ†æç‰ˆï¼‰
SerpAPIï¼ˆç„¡æ–™ãƒ—ãƒ©ãƒ³ï¼‰ã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢çµæœåˆ†æã‚’å®Ÿè¡Œ

ä½œæˆæ—¥: 2025å¹´1æœˆ27æ—¥
æ›´æ–°æ—¥: 2025å¹´1æœˆ27æ—¥
æ©Ÿèƒ½: ç«¶åˆSERPåˆ†æã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼šç™ºè¦‹ã€å‹•çš„ãƒ‡ãƒ¼ã‚¿å–å¾—
"""

import requests
import json
import time
from datetime import datetime
import os
from dotenv import load_dotenv
from urllib.parse import urlparse
import re

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

class FuneralSEOAnalyzer:
    def __init__(self, api_key):
        """
        SerpAPIï¼ˆç„¡æ–™ãƒ—ãƒ©ãƒ³ï¼‰ã‚’ä½¿ç”¨ã—ãŸè‘¬å„€æ¥­ç•ŒSEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚¯ãƒ©ã‚¹
        å‹•çš„SERPåˆ†æãƒ»ç«¶åˆåˆ†ææ©Ÿèƒ½ä»˜ã
        """
        self.api_key = api_key
        self.base_url = "https://serpapi.com/search"
        self.target_location = os.getenv("TARGET_LOCATION", "ç¥å¥ˆå·çœŒæ¨ªæµœå¸‚")
        self.company_domain = os.getenv("COMPANY_DOMAIN", "wakousougisya.com")
        
        # ç«¶åˆãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè‘¬å„€æ¥­ç•Œä¸»è¦ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼‰
        self.competitor_patterns = {
            "å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³": ["aeon-life.jp", "koekisha.co.jp", "e-sogi.com", "sougi-sos.com"],
            "åœ°åŸŸå¯†ç€": ["yokohama", "kanagawa", "sougi"],
            "æ¯”è¼ƒãƒãƒ¼ã‚¿ãƒ«": ["iisogi.com", "chiisanaososhiki.jp", "osohshiki.jp", "sogi.jp"],
            "æƒ…å ±ã‚µã‚¤ãƒˆ": ["syukatsulabo.jp", "osohshiki-plaza.com", "sougi-guide"]
        }
        
    def get_search_results(self, keyword, location=None):
        """æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œç´¢çµæœã‚’è©³ç´°å–å¾—"""
        if location is None:
            location = self.target_location
            
        params = {
            "engine": "google",
            "q": keyword,
            "location": location,
            "hl": "ja",
            "gl": "jp",
            "api_key": self.api_key,
            "num": 100  # ã‚ˆã‚Šå¤šãã®çµæœã‚’å–å¾—
        }
        
        try:
            response = requests.get(self.base_url, params=params)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"  âš ï¸ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return None
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {keyword}ã®æ¤œç´¢çµæœå–å¾—ã«å¤±æ•— - {e}")
            return None
    
    def analyze_funeral_keywords_dynamic(self):
        """å’Œå…‰è‘¬å„€ç¤¾å‘ã‘å‹•çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æå®Ÿè¡Œ"""
        
        # æ‹¡å¼µã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–æ‹ ï¼‰
        target_keywords = {
            "ãƒ¡ã‚¤ãƒ³ï¼ˆå„ªå…ˆåº¦ï¼šé«˜ï¼‰": [
                "è‘¬å„€ æ¨ªæµœ",
                "å®¶æ—è‘¬ ç¥å¥ˆå·", 
                "è‘¬å„€ç¤¾ æ¨ªæµœ",
                "ç›´è‘¬ è²»ç”¨"
            ],
            "åœ°åŸŸç‰¹åŒ–ï¼ˆå„ªå…ˆåº¦ï¼šä¸­ï¼‰": [
                "è‘¬å„€ æ¸¯åŒ—åŒº",
                "è‘¬å„€ç¤¾ ç¥å¥ˆå·åŒº",
                "ç«è‘¬å¼ æ¨ªæµœ"
            ],
            "å·®åˆ¥åŒ–ï¼ˆå„ªå…ˆåº¦ï¼šä¸­ï¼‰": [
                "è‘¬å„€ è¿½åŠ æ–™é‡‘ãªã— æ¨ªæµœ",
                "24æ™‚é–“å¯¾å¿œ è‘¬å„€ç¤¾ ç¥å¥ˆå·",
                "è‘¬ç¥­ãƒ‡ã‚£ãƒ¬ã‚¯ã‚¿ãƒ¼ è³‡æ ¼è€… æ¨ªæµœ"
            ]
        }
        
        all_keywords = []
        for category, keywords in target_keywords.items():
            all_keywords.extend(keywords)
        
        analysis_results = {
            "åˆ†æå®Ÿè¡Œæƒ…å ±": {
                "åˆ†ææ—¥æ™‚": datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S"),
                "å¯¾è±¡åœ°åŸŸ": self.target_location,
                "åˆ†æå¯¾è±¡ä¼æ¥­": "å’Œå…‰è‘¬å„€ç¤¾",
                "ä½¿ç”¨API": "SerpAPIï¼ˆå‹•çš„åˆ†æç‰ˆï¼‰",
                "åˆ†æã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°": len(all_keywords),
                "ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸç‡": "0/0 (0%)"
            },
            "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é †ä½ãƒ»ç«¶åˆåˆ†æ": {
                "å’Œå…‰è‘¬å„€ç¤¾é †ä½çŠ¶æ³": {},
                "ç«¶åˆSERPåˆ†æçµæœ": {
                    "ä¸»è¦ç«¶åˆãƒ‘ã‚¿ãƒ¼ãƒ³": {},
                    "SERPæ©Ÿèƒ½åˆ†æ": {}
                }
            },
            "å‹•çš„ç™ºè¦‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼š": {
                "æ–°è¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": [],
                "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—åˆ†æçµæœ": {}
            },
            "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {}
        }
        
        print("=" * 70)
        print("ğŸ” å’Œå…‰è‘¬å„€ç¤¾ SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‹•çš„åˆ†æé–‹å§‹")
        print("=" * 70)
        print(f"ğŸ“… åˆ†ææ—¥æ™‚: {analysis_results['åˆ†æå®Ÿè¡Œæƒ…å ±']['åˆ†ææ—¥æ™‚']}")
        print(f"ğŸ“ å¯¾è±¡åœ°åŸŸ: {analysis_results['åˆ†æå®Ÿè¡Œæƒ…å ±']['å¯¾è±¡åœ°åŸŸ']}")
        print(f"ğŸ¯ åˆ†æå¯¾è±¡: {len(all_keywords)}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ3ã‚«ãƒ†ã‚´ãƒªï¼‰")
        print("ğŸ“Š æ©Ÿèƒ½: ç«¶åˆSERPåˆ†æã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼šç™ºè¦‹ã€SERPæ©Ÿèƒ½åˆ†æ")
        print("=" * 70)
        
        successful_analyses = 0
        wako_found_count = 0
        all_related_keywords = set()
        competitor_analysis = {}
        serp_features = {"local_pack": 0, "ads": 0, "paa": 0, "knowledge_graph": 0}
        
        for category, keywords in target_keywords.items():
            print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒª: {category}")
            print("-" * 50)
            
            for i, keyword in enumerate(keywords, 1):
                print(f"\n[{successful_analyses + 1}/{len(all_keywords)}] ğŸ” åˆ†æä¸­: {keyword}")
                
                # APIåˆ¶é™å›é¿ã®ãŸã‚å¾…æ©Ÿ
                if successful_analyses > 0:
                    time.sleep(3)
                
                serp_data = self.get_search_results(keyword)
                
                if serp_data:
                    # è©³ç´°åˆ†æå®Ÿè¡Œ
                    keyword_analysis = self.extract_comprehensive_insights(keyword, serp_data)
                    analysis_results["è©³ç´°ãƒ‡ãƒ¼ã‚¿"][keyword] = keyword_analysis
                    
                    # å’Œå…‰è‘¬å„€ç¤¾é †ä½è¨˜éŒ²
                    analysis_results["ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é †ä½ãƒ»ç«¶åˆåˆ†æ"]["å’Œå…‰è‘¬å„€ç¤¾é †ä½çŠ¶æ³"][keyword] = {
                        "é †ä½": keyword_analysis["å’Œå…‰è‘¬å„€ç¤¾ã®é †ä½"],
                        "ä¸Šä½3ã‚µã‚¤ãƒˆ": keyword_analysis["ä¸Šä½ç«¶åˆã‚µã‚¤ãƒˆ"][:3] if keyword_analysis["ä¸Šä½ç«¶åˆã‚µã‚¤ãƒˆ"] else []
                    }
                    
                    # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åé›†
                    all_related_keywords.update(keyword_analysis["é–¢é€£æ¤œç´¢"])
                    
                    # ç«¶åˆåˆ†æãƒ‡ãƒ¼ã‚¿è“„ç©
                    self.accumulate_competitor_data(keyword_analysis, competitor_analysis)
                    
                    # SERPæ©Ÿèƒ½ã‚«ã‚¦ãƒ³ãƒˆ
                    self.count_serp_features(keyword_analysis, serp_features)
                    
                    # çµæœã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
                    self.print_detailed_keyword_results(keyword, keyword_analysis)
                    successful_analyses += 1
                    
                    if keyword_analysis["å’Œå…‰è‘¬å„€ç¤¾ã®é †ä½"] != "åœå¤–":
                        wako_found_count += 1
                else:
                    print(f"  âŒ {keyword}ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # åˆ†æçµæœã®çµ±åˆãƒ»ã‚µãƒãƒªãƒ¼ä½œæˆ
        analysis_results["åˆ†æå®Ÿè¡Œæƒ…å ±"]["ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸç‡"] = f"{successful_analyses}/{len(all_keywords)} ({(successful_analyses/len(all_keywords)*100):.1f}%)"
        
        # ç«¶åˆåˆ†æçµæœã®æ•´ç†
        analysis_results["ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é †ä½ãƒ»ç«¶åˆåˆ†æ"]["ç«¶åˆSERPåˆ†æçµæœ"] = {
            "ä¸»è¦ç«¶åˆãƒ‘ã‚¿ãƒ¼ãƒ³": self.analyze_competitor_patterns(competitor_analysis),
            "SERPæ©Ÿèƒ½åˆ†æ": self.format_serp_features(serp_features, successful_analyses)
        }
        
        # æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼šã®åˆ†æ
        analysis_results["å‹•çš„ç™ºè¦‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼š"] = self.discover_keyword_opportunities(all_related_keywords, analysis_results["è©³ç´°ãƒ‡ãƒ¼ã‚¿"])
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        self.save_comprehensive_results(analysis_results)
        self.print_comprehensive_summary(analysis_results)
        
        return analysis_results
    
    def extract_comprehensive_insights(self, keyword, serp_data):
        """æ¤œç´¢çµæœã‹ã‚‰åŒ…æ‹¬çš„ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’æŠ½å‡ºï¼ˆSERPæ©Ÿèƒ½åˆ†æå«ã‚€ï¼‰"""
        insights = {
            "ç«¶åˆã‚µã‚¤ãƒˆæ•°": 0,
            "ä¸Šä½ç«¶åˆã‚µã‚¤ãƒˆ": [],
            "é–¢é€£æ¤œç´¢": [],
            "åºƒå‘Šå‡ºç¨¿çŠ¶æ³": "ç„¡ã—",
            "å’Œå…‰è‘¬å„€ç¤¾ã®é †ä½": "åœå¤–",
            "æ¤œç´¢çµæœã®ç‰¹å¾´": [],
            "SERPæ©Ÿèƒ½": {
                "local_pack": False,
                "ads_count": 0,
                "paa_questions": [],
                "knowledge_graph": False,
                "featured_snippet": False
            },
            "ç«¶åˆåˆ†é¡": {"å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³": 0, "åœ°åŸŸå¯†ç€": 0, "æ¯”è¼ƒãƒãƒ¼ã‚¿ãƒ«": 0, "æƒ…å ±ã‚µã‚¤ãƒˆ": 0, "ãã®ä»–": 0}
        }
        
        # ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯æ¤œç´¢çµæœã®è©³ç´°åˆ†æ
        if "organic_results" in serp_data:
            insights["ç«¶åˆã‚µã‚¤ãƒˆæ•°"] = len(serp_data["organic_results"])
            
            # ä¸Šä½20ã‚µã‚¤ãƒˆã‚’è©³ç´°åˆ†æ
            for i, result in enumerate(serp_data["organic_results"][:20]):
                site_info = {
                    "é †ä½": i + 1,
                    "ã‚¿ã‚¤ãƒˆãƒ«": result.get("title", ""),
                    "URL": result.get("link", ""),
                    "ãƒ‰ãƒ¡ã‚¤ãƒ³": self.extract_domain(result.get("link", "")),
                    "ã‚¹ãƒ‹ãƒšãƒƒãƒˆ": result.get("snippet", "")[:100] + "..." if result.get("snippet") else "",
                    "ç«¶åˆã‚«ãƒ†ã‚´ãƒª": self.classify_competitor(result.get("link", ""))
                }
                insights["ä¸Šä½ç«¶åˆã‚µã‚¤ãƒˆ"].append(site_info)
                
                # ç«¶åˆåˆ†é¡ã‚«ã‚¦ãƒ³ãƒˆ
                category = site_info["ç«¶åˆã‚«ãƒ†ã‚´ãƒª"]
                insights["ç«¶åˆåˆ†é¡"][category] += 1
                
                # å’Œå…‰è‘¬å„€ç¤¾ã®é †ä½ãƒã‚§ãƒƒã‚¯
                if self.company_domain in result.get("link", "").lower():
                    insights["å’Œå…‰è‘¬å„€ç¤¾ã®é †ä½"] = f"{i + 1}ä½"
        
        # é–¢é€£æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ‹¡å¼µï¼‰
        if "related_searches" in serp_data:
            insights["é–¢é€£æ¤œç´¢"] = [
                item.get("query", "") for item in serp_data["related_searches"]
            ]
        
        # People Also Askåˆ†æ
        if "people_also_ask" in serp_data:
            insights["SERPæ©Ÿèƒ½"]["paa_questions"] = [
                paa.get("question", "") for paa in serp_data["people_also_ask"]
            ]
        
        # åºƒå‘Šã®è©³ç´°åˆ†æ
        if "ads" in serp_data and len(serp_data["ads"]) > 0:
            insights["åºƒå‘Šå‡ºç¨¿çŠ¶æ³"] = f"{len(serp_data['ads'])}ä»¶ã®åºƒå‘Šã‚ã‚Š"
            insights["SERPæ©Ÿèƒ½"]["ads_count"] = len(serp_data["ads"])
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ãƒƒã‚¯
        if "local_results" in serp_data:
            insights["SERPæ©Ÿèƒ½"]["local_pack"] = True
            insights["æ¤œç´¢çµæœã®ç‰¹å¾´"].append("ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢çµæœã‚ã‚Š")
        
        # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•
        if "knowledge_graph" in serp_data:
            insights["SERPæ©Ÿèƒ½"]["knowledge_graph"] = True
            insights["æ¤œç´¢çµæœã®ç‰¹å¾´"].append("ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚ã‚Š")
        
        # ç‰¹é›†ã‚¹ãƒ‹ãƒšãƒƒãƒˆ
        if "featured_snippet" in serp_data:
            insights["SERPæ©Ÿèƒ½"]["featured_snippet"] = True
            insights["æ¤œç´¢çµæœã®ç‰¹å¾´"].append("ç‰¹é›†ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚ã‚Š")
        
        return insights
    
    def classify_competitor(self, url):
        """URLã‹ã‚‰ç«¶åˆã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡"""
        if not url:
            return "ãã®ä»–"
        
        url_lower = url.lower()
        
        for pattern in self.competitor_patterns["å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³"]:
            if pattern in url_lower:
                return "å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³"
        
        for pattern in self.competitor_patterns["æ¯”è¼ƒãƒãƒ¼ã‚¿ãƒ«"]:
            if pattern in url_lower:
                return "æ¯”è¼ƒãƒãƒ¼ã‚¿ãƒ«"
        
        for pattern in self.competitor_patterns["åœ°åŸŸå¯†ç€"]:
            if pattern in url_lower:
                return "åœ°åŸŸå¯†ç€"
        
        for pattern in self.competitor_patterns["æƒ…å ±ã‚µã‚¤ãƒˆ"]:
            if pattern in url_lower:
                return "æƒ…å ±ã‚µã‚¤ãƒˆ"
        
        return "ãã®ä»–"
    
    def accumulate_competitor_data(self, keyword_analysis, competitor_analysis):
        """ç«¶åˆãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©"""
        for site in keyword_analysis["ä¸Šä½ç«¶åˆã‚µã‚¤ãƒˆ"][:10]:  # ä¸Šä½10ä½ã¾ã§
            domain = site["ãƒ‰ãƒ¡ã‚¤ãƒ³"]
            if domain and self.company_domain not in domain:
                if domain not in competitor_analysis:
                    competitor_analysis[domain] = {
                        "å‡ºç¾å›æ•°": 0,
                        "å¹³å‡é †ä½": 0,
                        "é †ä½ãƒªã‚¹ãƒˆ": [],
                        "ã‚«ãƒ†ã‚´ãƒª": site["ç«¶åˆã‚«ãƒ†ã‚´ãƒª"],
                        "ã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³": []
                    }
                
                competitor_analysis[domain]["å‡ºç¾å›æ•°"] += 1
                competitor_analysis[domain]["é †ä½ãƒªã‚¹ãƒˆ"].append(site["é †ä½"])
                competitor_analysis[domain]["ã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³"].append(site["ã‚¿ã‚¤ãƒˆãƒ«"][:50])
                competitor_analysis[domain]["å¹³å‡é †ä½"] = sum(competitor_analysis[domain]["é †ä½ãƒªã‚¹ãƒˆ"]) / len(competitor_analysis[domain]["é †ä½ãƒªã‚¹ãƒˆ"])
    
    def count_serp_features(self, keyword_analysis, serp_features):
        """SERPæ©Ÿèƒ½ã®å‡ºç¾ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        if keyword_analysis["SERPæ©Ÿèƒ½"]["local_pack"]:
            serp_features["local_pack"] += 1
        if keyword_analysis["SERPæ©Ÿèƒ½"]["ads_count"] > 0:
            serp_features["ads"] += 1
        if keyword_analysis["SERPæ©Ÿèƒ½"]["paa_questions"]:
            serp_features["paa"] += 1
        if keyword_analysis["SERPæ©Ÿèƒ½"]["knowledge_graph"]:
            serp_features["knowledge_graph"] += 1
    
    def analyze_competitor_patterns(self, competitor_analysis):
        """ç«¶åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        # å‡ºç¾å›æ•°é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_competitors = sorted(
            competitor_analysis.items(), 
            key=lambda x: x[1]["å‡ºç¾å›æ•°"], 
            reverse=True
        )
        
        patterns = {}
        for domain, data in sorted_competitors[:10]:  # ä¸Šä½10ç«¶åˆ
            patterns[domain] = {
                "å‡ºç¾ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°": data["å‡ºç¾å›æ•°"],
                "å¹³å‡é †ä½": round(data["å¹³å‡é †ä½"], 1),
                "ã‚«ãƒ†ã‚´ãƒª": data["ã‚«ãƒ†ã‚´ãƒª"],
                "SEOæˆ¦ç•¥": self.analyze_title_patterns(data["ã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³"])
            }
        
        return patterns
    
    def analyze_title_patterns(self, titles):
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰SEOæˆ¦ç•¥ã‚’åˆ†æ"""
        if not titles:
            return "ä¸æ˜"
        
        # ã‚ˆãä½¿ã‚ã‚Œã‚‹è¨€è‘‰ã‚’æŠ½å‡º
        common_words = []
        for title in titles:
            words = re.findall(r'[ä¸€-é¾¯ã‚¡-ãƒ¶ãƒ¼]+', title)  # æ—¥æœ¬èªã®ã¿æŠ½å‡º
            common_words.extend(words)
        
        # é »å‡ºå˜èªTOP3
        from collections import Counter
        word_counts = Counter(common_words)
        top_words = [word for word, count in word_counts.most_common(3)]
        
        return f"ã‚ˆãä½¿ç”¨: {', '.join(top_words)}" if top_words else "ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸æ˜"
    
    def format_serp_features(self, serp_features, total_keywords):
        """SERPæ©Ÿèƒ½ã®åˆ†æçµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if total_keywords == 0:
            return {"ã‚¨ãƒ©ãƒ¼": "åˆ†æãƒ‡ãƒ¼ã‚¿ãªã—"}
        
        return {
            "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ãƒƒã‚¯è¡¨ç¤º": f"{serp_features['local_pack']}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ{(serp_features['local_pack']/total_keywords*100):.1f}%ï¼‰",
            "åºƒå‘Šè¡¨ç¤º": f"{serp_features['ads']}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ{(serp_features['ads']/total_keywords*100):.1f}%ï¼‰",
            "PAAè¡¨ç¤º": f"{serp_features['paa']}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ{(serp_features['paa']/total_keywords*100):.1f}%ï¼‰",
            "ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•": f"{serp_features['knowledge_graph']}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ{(serp_features['knowledge_graph']/total_keywords*100):.1f}%ï¼‰"
        }
    
    def discover_keyword_opportunities(self, all_related_keywords, detailed_data):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼šã‚’ç™ºè¦‹ãƒ»åˆ†æ"""
        # é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ–°è¦æ©Ÿä¼šã‚’ç‰¹å®š
        existing_keywords = set(detailed_data.keys())
        new_opportunities = []
        
        for related_kw in all_related_keywords:
            if related_kw not in existing_keywords and related_kw:
                # ç«¶åˆå¼·åº¦ã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
                strength = self.estimate_competition_strength(related_kw, detailed_data)
                new_opportunities.append({
                    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": related_kw,
                    "æ¤œç´¢æ„å›³": self.infer_search_intent(related_kw),
                    "ç«¶åˆå¼·åº¦": strength
                })
        
        # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        new_opportunities.sort(key=lambda x: self.calculate_opportunity_score(x), reverse=True)
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
        content_gaps = self.analyze_content_gaps(detailed_data)
        
        return {
            "æ–°è¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": new_opportunities[:10],  # TOP10
            "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—åˆ†æçµæœ": content_gaps
        }
    
    def estimate_competition_strength(self, keyword, detailed_data):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç«¶åˆå¼·åº¦ã‚’æ¨å®š"""
        # é¡ä¼¼ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ¨å®š
        funeral_terms = ["è‘¬å„€", "å®¶æ—è‘¬", "ç›´è‘¬", "ç«è‘¬"]
        if any(term in keyword for term in funeral_terms):
            return "ä¸­"
        elif "æ¨ªæµœ" in keyword or "ç¥å¥ˆå·" in keyword:
            return "å¼±"
        else:
            return "å¼·"
    
    def infer_search_intent(self, keyword):
        """æ¤œç´¢æ„å›³ã‚’æ¨æ¸¬"""
        if "è²»ç”¨" in keyword or "æ–™é‡‘" in keyword or "å®‰ã„" in keyword:
            return "ä¾¡æ ¼æƒ…å ±"
        elif "å£ã‚³ãƒŸ" in keyword or "è©•åˆ¤" in keyword:
            return "è©•ä¾¡ãƒ»æ¯”è¼ƒ"
        elif "æµã‚Œ" in keyword or "æ‰‹ç¶šã" in keyword:
            return "æƒ…å ±åé›†"
        elif "24æ™‚é–“" in keyword or "æ€¥" in keyword:
            return "ç·Šæ€¥å¯¾å¿œ"
        else:
            return "ã‚µãƒ¼ãƒ“ã‚¹æ¤œç´¢"
    
    def calculate_opportunity_score(self, opportunity):
        """æ©Ÿä¼šã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        intent_scores = {"ä¾¡æ ¼æƒ…å ±": 5, "ç·Šæ€¥å¯¾å¿œ": 4, "ã‚µãƒ¼ãƒ“ã‚¹æ¤œç´¢": 3, "è©•ä¾¡ãƒ»æ¯”è¼ƒ": 2, "æƒ…å ±åé›†": 1}
        strength_scores = {"å¼±": 3, "ä¸­": 2, "å¼·": 1}
        
        intent_score = intent_scores.get(opportunity["æ¤œç´¢æ„å›³"], 1)
        strength_score = strength_scores.get(opportunity["ç«¶åˆå¼·åº¦"], 1)
        
        return intent_score + strength_score
    
    def analyze_content_gaps(self, detailed_data):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—ã‚’åˆ†æ"""
        gaps = {
            "å’Œå…‰è‘¬å„€ç¤¾ãŒå„ªä½ã«ç«‹ã¦ã‚‹é ˜åŸŸ": [],
            "å³åº§ã«å¯¾å¿œã™ã¹ãæ¤œç´¢æ„å›³": []
        }
        
        # å’Œå…‰è‘¬å„€ç¤¾ãŒåœå¤–ã§ã€ç«¶åˆãŒå¼±ã„é ˜åŸŸã‚’ç‰¹å®š
        for keyword, data in detailed_data.items():
            if data["å’Œå…‰è‘¬å„€ç¤¾ã®é †ä½"] == "åœå¤–":
                # å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³ãŒå°‘ãªã„å ´åˆã¯æ©Ÿä¼š
                if data["ç«¶åˆåˆ†é¡"]["å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³"] <= 2:
                    gaps["å’Œå…‰è‘¬å„€ç¤¾ãŒå„ªä½ã«ç«‹ã¦ã‚‹é ˜åŸŸ"].append(f"{keyword}: å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³å°‘æ•°")
                
                # åœ°åŸŸå¯†ç€ãŒå¤šã„å ´åˆã‚‚æ©Ÿä¼š
                if data["ç«¶åˆåˆ†é¡"]["åœ°åŸŸå¯†ç€"] >= 3:
                    gaps["å³åº§ã«å¯¾å¿œã™ã¹ãæ¤œç´¢æ„å›³"].append(f"{keyword}: åœ°åŸŸç‰¹åŒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¼·åŒ–")
        
        return gaps
    
    def extract_domain(self, url):
        """URLã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’æŠ½å‡º"""
        try:
            return urlparse(url).netloc
        except:
            return None
    
    def print_detailed_keyword_results(self, keyword, analysis):
        """è©³ç´°ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æçµæœã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¡¨ç¤º"""
        print(f"  ğŸ“Š {keyword}ã®è©³ç´°åˆ†æçµæœ:")
        print(f"    ğŸ† å’Œå…‰è‘¬å„€ç¤¾é †ä½: {analysis['å’Œå…‰è‘¬å„€ç¤¾ã®é †ä½']}")
        print(f"    ğŸŒ ç«¶åˆã‚µã‚¤ãƒˆæ•°: {analysis['ç«¶åˆã‚µã‚¤ãƒˆæ•°']}ä»¶")
        print(f"    ğŸ“¢ åºƒå‘Š: {analysis['åºƒå‘Šå‡ºç¨¿çŠ¶æ³']}")
        
        # ç«¶åˆåˆ†é¡è¡¨ç¤º
        comp_class = analysis['ç«¶åˆåˆ†é¡']
        print(f"    ğŸ¢ ç«¶åˆå†…è¨³: å¤§æ‰‹{comp_class['å¤§æ‰‹ãƒã‚§ãƒ¼ãƒ³']}ãƒ»åœ°åŸŸ{comp_class['åœ°åŸŸå¯†ç€']}ãƒ»ãƒãƒ¼ã‚¿ãƒ«{comp_class['æ¯”è¼ƒãƒãƒ¼ã‚¿ãƒ«']}ãƒ»æƒ…å ±{comp_class['æƒ…å ±ã‚µã‚¤ãƒˆ']}ãƒ»ä»–{comp_class['ãã®ä»–']}")
        
        # SERPæ©Ÿèƒ½
        serp_feat = analysis['SERPæ©Ÿèƒ½']
        features = []
        if serp_feat['local_pack']:
            features.append("ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ãƒƒã‚¯")
        if serp_feat['ads_count'] > 0:
            features.append(f"åºƒå‘Š{serp_feat['ads_count']}ä»¶")
        if serp_feat['paa_questions']:
            features.append("PAA")
        if serp_feat['knowledge_graph']:
            features.append("ãƒŠãƒ¬ãƒƒã‚¸")
        
        print(f"    ğŸ¯ SERPæ©Ÿèƒ½: {', '.join(features) if features else 'ãªã—'}")
        
        # ä¸Šä½3ã‚µã‚¤ãƒˆ
        if analysis['ä¸Šä½ç«¶åˆã‚µã‚¤ãƒˆ']:
            print(f"    ğŸ¥‡ TOP3: ", end="")
            top3 = [f"{i+1}ä½:{site['ãƒ‰ãƒ¡ã‚¤ãƒ³']}" for i, site in enumerate(analysis['ä¸Šä½ç«¶åˆã‚µã‚¤ãƒˆ'][:3])]
            print(" | ".join(top3))
        
        # é–¢é€£æ¤œç´¢ï¼ˆæœ€åˆã®3ã¤ï¼‰
        if analysis['é–¢é€£æ¤œç´¢']:
            related = analysis['é–¢é€£æ¤œç´¢'][:3]
            print(f"    ğŸ”— é–¢é€£: {', '.join(related)}")
    
    def save_comprehensive_results(self, results):
        """åŒ…æ‹¬çš„ãªåˆ†æçµæœã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONå½¢å¼ã§è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        try:
            os.makedirs("../outputs", exist_ok=True)
            filename = f"../outputs/seo-analysis-dynamic_{timestamp}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ è©³ç´°åˆ†æçµæœã‚’ä¿å­˜: {filename}")
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        # Markdownå½¢å¼ã§ã‚µãƒãƒªãƒ¼ä¿å­˜
        try:
            md_filename = f"../outputs/seo-analysis-summary_{timestamp}.md"
            self.generate_markdown_report(results, md_filename)
            print(f"ğŸ“„ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {md_filename}")
        except Exception as e:
            print(f"âš ï¸ Markdownãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_markdown_report(self, results, filename):
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        info = results["åˆ†æå®Ÿè¡Œæƒ…å ±"]
        rank_info = results["ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é †ä½ãƒ»ç«¶åˆåˆ†æ"]
        opportunities = results["å‹•çš„ç™ºè¦‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼š"]
        
        md_content = f"""# å’Œå…‰è‘¬å„€ç¤¾ SEOåˆ†æãƒ»æ”¹å–„ææ¡ˆï¼ˆSerpAPIå‹•çš„åˆ†æçµæœï¼‰

## åˆ†æå®Ÿè¡Œæƒ…å ±
- **åˆ†ææ—¥æ™‚**: {info["åˆ†ææ—¥æ™‚"]}
- **ä½¿ç”¨API**: {info["ä½¿ç”¨API"]}
- **åˆ†æã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°**: {info["åˆ†æã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°"]}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
- **ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸç‡**: {info["ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸç‡"]}

## ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é †ä½ãƒ»ç«¶åˆåˆ†æ

### ç¾åœ¨ã®å’Œå…‰è‘¬å„€ç¤¾é †ä½çŠ¶æ³
| ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ | ç¾åœ¨é †ä½ | 1ä½ã‚µã‚¤ãƒˆ | 2ä½ã‚µã‚¤ãƒˆ | 3ä½ã‚µã‚¤ãƒˆ |
|------------|----------|-----------|-----------|-----------|
"""
        
        # é †ä½çŠ¶æ³ãƒ†ãƒ¼ãƒ–ãƒ«ç”Ÿæˆ
        for keyword, rank_data in rank_info["å’Œå…‰è‘¬å„€ç¤¾é †ä½çŠ¶æ³"].items():
            rank = rank_data["é †ä½"]
            top3 = rank_data["ä¸Šä½3ã‚µã‚¤ãƒˆ"]
            
            site1 = top3[0]["ãƒ‰ãƒ¡ã‚¤ãƒ³"] if len(top3) > 0 else "-"
            site2 = top3[1]["ãƒ‰ãƒ¡ã‚¤ãƒ³"] if len(top3) > 1 else "-"
            site3 = top3[2]["ãƒ‰ãƒ¡ã‚¤ãƒ³"] if len(top3) > 2 else "-"
            
            md_content += f"| {keyword} | {rank} | {site1} | {site2} | {site3} |\n"
        
        # ç«¶åˆåˆ†æçµæœ
        comp_patterns = rank_info["ç«¶åˆSERPåˆ†æçµæœ"]["ä¸»è¦ç«¶åˆãƒ‘ã‚¿ãƒ¼ãƒ³"]
        md_content += f"""
### ç«¶åˆSERPåˆ†æçµæœ

#### ä¸»è¦ç«¶åˆãƒ‘ã‚¿ãƒ¼ãƒ³
"""
        
        for i, (domain, data) in enumerate(comp_patterns.items(), 1):
            md_content += f"""
{i}. **{domain}**: {data["å‡ºç¾ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°"]}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ä¸Šä½è¡¨ç¤ºï¼ˆå¹³å‡{data["å¹³å‡é †ä½"]}ä½ï¼‰
   - ã‚«ãƒ†ã‚´ãƒª: {data["ã‚«ãƒ†ã‚´ãƒª"]}
   - SEOæˆ¦ç•¥: {data["SEOæˆ¦ç•¥"]}
"""
        
        # SERPæ©Ÿèƒ½åˆ†æ
        serp_features = rank_info["ç«¶åˆSERPåˆ†æçµæœ"]["SERPæ©Ÿèƒ½åˆ†æ"]
        md_content += f"""
#### SERPæ©Ÿèƒ½åˆ†æ
- **ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ãƒƒã‚¯è¡¨ç¤º**: {serp_features.get("ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ãƒƒã‚¯è¡¨ç¤º", "ãƒ‡ãƒ¼ã‚¿ãªã—")}
- **åºƒå‘Šè¡¨ç¤º**: {serp_features.get("åºƒå‘Šè¡¨ç¤º", "ãƒ‡ãƒ¼ã‚¿ãªã—")}
- **PAAè¡¨ç¤º**: {serp_features.get("PAAè¡¨ç¤º", "ãƒ‡ãƒ¼ã‚¿ãªã—")}
- **ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•**: {serp_features.get("ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•", "ãƒ‡ãƒ¼ã‚¿ãªã—")}
"""
        
        # æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼š
        new_keywords = opportunities["æ–°è¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"]
        md_content += f"""
## å‹•çš„ç™ºè¦‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼š

### æ–°è¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé–¢é€£æ¤œç´¢ã‹ã‚‰ç™ºè¦‹ï¼‰
"""
        
        for i, kw_data in enumerate(new_keywords, 1):
            md_content += f"{i}. **{kw_data['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰']}** - æ¤œç´¢æ„å›³: {kw_data['æ¤œç´¢æ„å›³']} - ç«¶åˆå¼·åº¦: {kw_data['ç«¶åˆå¼·åº¦']}\n"
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—
        gaps = opportunities["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—åˆ†æçµæœ"]
        md_content += f"""
### ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—åˆ†æçµæœ

#### å’Œå…‰è‘¬å„€ç¤¾ãŒå„ªä½ã«ç«‹ã¦ã‚‹é ˜åŸŸ
"""
        for gap in gaps["å’Œå…‰è‘¬å„€ç¤¾ãŒå„ªä½ã«ç«‹ã¦ã‚‹é ˜åŸŸ"]:
            md_content += f"- {gap}\n"
        
        md_content += f"""
#### å³åº§ã«å¯¾å¿œã™ã¹ãæ¤œç´¢æ„å›³
"""
        for intent in gaps["å³åº§ã«å¯¾å¿œã™ã¹ãæ¤œç´¢æ„å›³"]:
            md_content += f"1. {intent}\n"
        
        md_content += f"""
---
*ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯SerpAPIãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆ{info["åˆ†ææ—¥æ™‚"]}ï¼‰ã«åŸºã¥ãåˆ†æçµæœã§ã™*
"""
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(md_content)
    
    def print_comprehensive_summary(self, results):
        """åŒ…æ‹¬çš„ãªã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "=" * 70)
        print("ğŸ“ˆ å’Œå…‰è‘¬å„€ç¤¾ SEOå‹•çš„åˆ†æ å®Œäº†ã‚µãƒãƒªãƒ¼")
        print("=" * 70)
        
        info = results["åˆ†æå®Ÿè¡Œæƒ…å ±"]
        print(f"ğŸ“… åˆ†æå®Œäº†: {info['åˆ†ææ—¥æ™‚']}")
        print(f"ğŸ¯ åˆ†ææˆåŠŸç‡: {info['ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸç‡']}")
        
        # é †ä½çŠ¶æ³ã‚µãƒãƒªãƒ¼
        rank_info = results["ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é †ä½ãƒ»ç«¶åˆåˆ†æ"]["å’Œå…‰è‘¬å„€ç¤¾é †ä½çŠ¶æ³"]
        ranked_count = sum(1 for data in rank_info.values() if data["é †ä½"] != "åœå¤–")
        print(f"ğŸ† ãƒ©ãƒ³ã‚¯ã‚¤ãƒ³: {ranked_count}/{len(rank_info)}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
        
        # æ–°è¦æ©Ÿä¼š
        new_kw_count = len(results["å‹•çš„ç™ºè¦‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼š"]["æ–°è¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"])
        print(f"ğŸ” æ–°è¦æ©Ÿä¼š: {new_kw_count}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç™ºè¦‹")
        
        # ç«¶åˆçŠ¶æ³
        comp_patterns = results["ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é †ä½ãƒ»ç«¶åˆåˆ†æ"]["ç«¶åˆSERPåˆ†æçµæœ"]["ä¸»è¦ç«¶åˆãƒ‘ã‚¿ãƒ¼ãƒ³"]
        top_competitor = list(comp_patterns.keys())[0] if comp_patterns else "ä¸æ˜"
        print(f"ğŸ¢ ä¸»è¦ç«¶åˆ: {top_competitor}")
        
        print("=" * 70)
        print("ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. ç”Ÿæˆã•ã‚ŒãŸMarkdownãƒ¬ãƒãƒ¼ãƒˆã§ã®è©³ç´°ç¢ºèª")
        print("  2. æ–°è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å„ªå…ˆé †ä½ä»˜ã‘")
        print("  3. ç«¶åˆã‚®ãƒ£ãƒƒãƒ—ã‚’çªãã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¼ç”»")
        print("  4. å®šæœŸå®Ÿè¡Œï¼ˆæœˆ1å›ï¼‰ã§ã®é †ä½å¤‰å‹•ç›£è¦–")
        print("=" * 70)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ å’Œå…‰è‘¬å„€ç¤¾ SEOã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‹•çš„åˆ†æãƒ„ãƒ¼ãƒ«")
    print("ğŸ“¡ SerpAPIä½¿ç”¨ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç«¶åˆåˆ†æãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ©Ÿä¼šç™ºè¦‹")
    print("")
    
    # APIã‚­ãƒ¼ç¢ºèª
    api_key = os.getenv("SERPAPI_KEY")
    if not api_key:
        print("âŒ ã‚¨ãƒ©ãƒ¼: .envãƒ•ã‚¡ã‚¤ãƒ«ã®SERPAPI_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ğŸ“ setupæ‰‹é †:")
        print("  1. env-template.txt ã‚’ .env ã«ã‚³ãƒ”ãƒ¼")
        print("  2. SerpAPIã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆä½œæˆãƒ»APIã‚­ãƒ¼å–å¾—")
        print("  3. .envãƒ•ã‚¡ã‚¤ãƒ«ã«APIã‚­ãƒ¼ã‚’è¨­å®š")
        print("  4. python test-serpapi.py ã§æ¥ç¶šãƒ†ã‚¹ãƒˆ")
        return
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = FuneralSEOAnalyzer(api_key)
    
    try:
        print("â³ å‹•çš„SEOåˆ†æã‚’é–‹å§‹...")
        results = analyzer.analyze_funeral_keywords_dynamic()
        
        print("\nâœ… åˆ†æå®Œäº†ï¼")
        print(f"ğŸ“Š çµæœãƒ•ã‚¡ã‚¤ãƒ«: outputs/ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèª")
        print(f"ğŸ”„ å®šæœŸå®Ÿè¡Œæ¨å¥¨: æœˆ1å›ã®ç«¶åˆç›£è¦–")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ åˆ†æã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ğŸ”§ å¯¾å‡¦æ–¹æ³•:")
        print("  1. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèª")
        print("  2. SerpAPIã‚­ãƒ¼ã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª")
        print("  3. APIåˆ¶é™å›æ•°ã‚’ç¢ºèª")
    
    print("\n" + "=" * 50)
    input("Enterã‚­ãƒ¼ã§çµ‚äº†...")

if __name__ == "__main__":
    main() 